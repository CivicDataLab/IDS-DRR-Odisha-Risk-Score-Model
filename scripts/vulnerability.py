import numpy as np
import DEA
import pandas as pd
import os
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import jenkspy


master_variables = pd.read_csv(os.getcwd()+'/flood-data-ecosystem-Odisha/RiskScoreModel/data/MASTER_VARIABLES.csv')
master_variables_copy = master_variables.copy()

# Per capita variables
#master_variables['Population_affected_Total'] = master_variables['Population_affected_Total']/master_variables['sum_population']
#master_variables['Total_Animal_Affected'] = master_variables['Total_Animal_Affected']/master_variables['sum_population']

#master_variables['Total_House_Fully_Damaged'] = master_variables['Total_House_Fully_Damaged']/master_variables['sum_population']
#master_variables['Crop_Area'] = master_variables['Crop_Area']/master_variables['net_sown_area_in_hac']
#master_variables['Roads'] = master_variables['Roads']/master_variables['rc_area']
#master_variables['Bridge'] = master_variables['Bridge']/master_variables['rc_area']
#master_variables['Embankment breached'] = master_variables['Embankment breached']/master_variables['rc_area']

master_variables['sum_young_population'] = master_variables['sum_young_population']/master_variables['sum_population']
master_variables['sum_aged_population'] = master_variables['sum_aged_population']/master_variables['sum_population']
master_variables['rail_length'] = master_variables['rail_length']/master_variables['block_area']
master_variables['schools_count'] = master_variables['schools_count']/master_variables['block_area']
master_variables['HealthCenters'] = master_variables['HealthCenters']/master_variables['block_area']
master_variables['road_length'] = master_variables['road_length']/master_variables['block_area']
master_variables['net_sown_area_in_hac'] = master_variables['net_sown_area_in_hac']/master_variables['block_area']
#master_variables['Embankments affected'] = master_variables['Embankments affected']/master_variables['rc_area']

#INPUT VARS
vulnerability_vars = ["mean_sex_ratio",
                      "sum_aged_population",
                      "avg_electricity",
                      "block_nosanitation_hhds_pct",
                      "block_piped_hhds_pct"]

#OUTPUT VARS
damage_vars = ["Total_Animal_Affected","Population_affected_Total",
                "Crop_Area","Total_House_Fully_Damaged","Embankments affected",
                 "Roads","Bridge"]

master_variables[damage_vars] = 1

vulnerability_df = master_variables[vulnerability_vars + damage_vars + ['timeperiod', 'object_id']]


# Function to assign bins based on breaks
def assign_bin(value):
    for i in range(len(breaks)):
        if value <= breaks[i]:
            return i + 1  # Since bins start from 1
    return len(breaks)  # If value is greater than the last break

vulnerability_df_months = []
for month in tqdm(vulnerability_df.timeperiod.unique()):
    vulnerability_df_month = vulnerability_df[vulnerability_df.timeperiod == month]
    vulnerability_df_month = vulnerability_df_month.set_index('object_id')
    # Initialize MinMaxScaler
    scaler = MinMaxScaler()
    # Fit scaler to the data and transform it
    vulnerability_df_month[vulnerability_vars + damage_vars] = scaler.fit_transform(vulnerability_df_month[vulnerability_vars + damage_vars])
    #vulnerability_df_month[vulnerability_vars] = scaler.fit_transform(vulnerability_df_month[vulnerability_vars])

    # Reversing a few input vars (as more input should be more vulnerability)
    #vulnerability_df_month['schools_count'] = 1 - vulnerability_df_month['schools_count']
    #vulnerability_df_month['HealthCenters'] = 1 - vulnerability_df_month['HealthCenters']
    #vulnerability_df_month['rail_length'] = 1 - vulnerability_df_month['rail_length']
    #vulnerability_df_month['road_length'] = 1 - vulnerability_df_month['road_length']
    vulnerability_df_month['avg_electricity'] = 1 - vulnerability_df_month['avg_electricity']
    vulnerability_df_month['block_piped_hhds_pct'] = 1 - vulnerability_df_month['block_piped_hhds_pct']
    
    # Reversing all output vars (as more output should be less damage)
    vulnerability_df_month[damage_vars] = 1 #- vulnerability_df_month[damage_vars]

    # Input dict
    X = vulnerability_df_month[vulnerability_vars].T.to_dict('list')

    # Output dict
    y = vulnerability_df_month[damage_vars].T.to_dict('list')

    DMU = list(vulnerability_df_month.index)#.astype(int))

    df = DEA.CRS(DMU, X, y, orientation="input", dual=False)


    vulnerability_df_month = vulnerability_df_month.reset_index().merge(df,
                                                          left_on = 'object_id',
                                                          right_on = 'DMU')
    print(vulnerability_df_month)
    #vulnerability_df_month.to_csv(os.getcwd()+'/flood-data-ecosystem-Odisha/RiskScoreModel/data/vul_debug.csv', index=False)
    vulnerability_df_month['efficiency'] = vulnerability_df_month['efficiency'].astype(float)

    vulnerability_df_month['efficiency'] = np.round(vulnerability_df_month['efficiency'], 8)# Perform Natural Jenks classification with 5 classes
    breaks = jenkspy.jenks_breaks(vulnerability_df_month['efficiency'], n_classes=5)

    #print(breaks)

    # Add a new column with the assigned bins
    vulnerability_df_month['vulnerability'] = pd.cut(vulnerability_df_month['efficiency'],
                                                     bins=breaks,
                                                     labels=[5, 4, 3, 2, 1], #Low efficiency = More Vulnerability
                                                     include_lowest=True)
    
    vulnerability_df_months.append(vulnerability_df_month)

vulnerability = pd.concat(vulnerability_df_months)

master_variables = master_variables_copy.merge(vulnerability[['timeperiod', 'object_id', 'efficiency', 'vulnerability']],
                       on = ['timeperiod', 'object_id'])

master_variables.to_csv(os.getcwd()+'/flood-data-ecosystem-Odisha/RiskScoreModel/data/factor_scores_l1_vulnerability.csv', index=False)



