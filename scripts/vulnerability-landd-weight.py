import numpy as np
import DEA
import pandas as pd
import os
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import jenkspy


master_variables = pd.read_csv(r'D:\CivicDataLab_IDS-DRR\IDS-DRR_Github\IDS-DRR-Assam\RiskScoreModel\data\MASTER_VARIABLES.csv')
master_variables_copy = master_variables.copy()

# Per capita variables
master_variables['Population_affected_Total'] = master_variables['Population_affected_Total']/master_variables['sum_population']
master_variables['Total_Animal_Affected'] = master_variables['Total_Animal_Affected']/master_variables['sum_population']
master_variables['Human_Live_Lost'] = master_variables['Human_Live_Lost']/master_variables['sum_population']


master_variables['Total_House_Fully_Damaged'] = master_variables['Total_House_Fully_Damaged']/master_variables['sum_population']
master_variables['Crop_Area'] = master_variables['Crop_Area']/master_variables['net_sown_area_in_hac']
master_variables['Roads'] = master_variables['Roads']/master_variables['rc_area']
master_variables['Bridge'] = master_variables['Bridge']/master_variables['rc_area']
master_variables['Embankment breached'] = master_variables['Embankment breached']/master_variables['rc_area']


master_variables['sum_aged_population'] = master_variables['sum_aged_population']/master_variables['rc_area']
master_variables['rail_length'] = master_variables['rail_length']/master_variables['rc_area']
master_variables['schools_count'] = master_variables['schools_count']/master_variables['rc_area']
master_variables['health_centres_count'] = master_variables['health_centres_count']/master_variables['rc_area']
master_variables['road_length'] = master_variables['road_length']/master_variables['rc_area']
master_variables['net_sown_area_in_hac'] = master_variables['net_sown_area_in_hac']/master_variables['rc_area']
master_variables['Embankments affected'] = master_variables['Embankments affected']/master_variables['rc_area']

#INPUT VARS
vulnerability_vars = ["mean_sexratio",
                      "schools_count",
                      "health_centres_count",
                      "rail_length",
                      "road_length",
                      "net_sown_area_in_hac",
                      "avg_electricity",
                      "rc_piped_hhds_pct",
                      
                      "rc_nosanitation_hhds_pct",
                      "sum_aged_population",
                      "Embankment breached",
                     ]

#OUTPUT VARS
damage_vars = [#"Total_Animal_Affected","Total_House_Fully_Damaged"
               "Human_Live_Lost","Population_affected_Total", "Crop_Area","Embankments affected",
                 "Roads","Bridge"]


# Apply custom weights based on damage conditions
vulnerability_df = master_variables[vulnerability_vars + damage_vars + ['timeperiod', 'object_id',]]

scaler = MinMaxScaler()
    # Fit scaler to the data and transform it
vulnerability_df['landd_score'] = scaler.fit_transform(vulnerability_df[damage_vars]).sum(axis=1) + 1

neg_inputs = ["rc_nosanitation_hhds_pct","sum_aged_population","Embankment breached"]

def apply_custom_weights(row, threshold=0.001):
    # Custom weights multiplier for significant damage
    #custom_weight = row['landd_score']  # Adjust this value as needed
    custom_weight = np.power(row['landd_score'], 2)
    # Check if any damage variable exceeds the threshold
    if any(row[damage_vars] > threshold):
        # Apply custom weights to the outputs (damage vars)
        row[damage_vars + neg_inputs] *= custom_weight

    return row



def assign_bin(value, breaks):
    for i in range(len(breaks)):
        if value <= breaks[i]:
            return i + 1  # Since bins start from 1
    return len(breaks)  # If value is greater than the last break


vulnerability_df_months = []
for month in tqdm(vulnerability_df.timeperiod.unique()):
    
    #print(vulnerability_df_month['landd_score'])
    vulnerability_df = vulnerability_df.apply(apply_custom_weights, axis=1)
    vulnerability_df_month = vulnerability_df[vulnerability_df.timeperiod == month]
    vulnerability_df_month = vulnerability_df_month.set_index('object_id')
    
    # Initialize MinMaxScaler
    #scaler = MinMaxScaler()
    # Fit scaler to the data and transform it
    vulnerability_df_month[vulnerability_vars + damage_vars] = scaler.fit_transform(vulnerability_df_month[vulnerability_vars + damage_vars])

    # Reverse certain input variables (as more input means more vulnerability)
    vulnerability_df_month['schools_count'] = 1 - vulnerability_df_month['schools_count']
    vulnerability_df_month['health_centres_count'] = 1 - vulnerability_df_month['health_centres_count']
    vulnerability_df_month['rail_length'] = 1 - vulnerability_df_month['rail_length']
    vulnerability_df_month['road_length'] = 1 - vulnerability_df_month['road_length']
    vulnerability_df_month['avg_electricity'] = 1 - vulnerability_df_month['avg_electricity']
    vulnerability_df_month['rc_piped_hhds_pct'] = 1 - vulnerability_df_month['rc_piped_hhds_pct']

    # Reverse all output (damage) variables (as more output means less damage)
    vulnerability_df_month[damage_vars] = 1 - vulnerability_df_month[damage_vars]

    # Input dict
    X = vulnerability_df_month[vulnerability_vars].T.to_dict('list')

    # Output dict
    y = vulnerability_df_month[damage_vars].T.to_dict('list')

    DMU = list(vulnerability_df_month.index.astype(int))

    df = DEA.CRS(DMU, X, y, orientation="input", dual=False)

    # Merge efficiency results
    vulnerability_df_month = vulnerability_df_month.reset_index().merge(df,
                                                                        left_on='object_id',
                                                                        right_on='DMU')
    
    # Perform Natural Jenks classification with 5 classes
    breaks = jenkspy.jenks_breaks(vulnerability_df_month['efficiency'], n_classes=5)

    # Add a new column with the assigned bins
    vulnerability_df_month['vulnerability'] = pd.cut(vulnerability_df_month['efficiency'],
                                                     bins=breaks,
                                                     labels=[5, 4, 3, 2, 1],  # Low efficiency = More Vulnerability
                                                     include_lowest=True)

    vulnerability_df_months.append(vulnerability_df_month)

vulnerability = pd.concat(vulnerability_df_months)
#ulnerability['landd'] = vulnerability[damage_vars].sum(axis=1)

master_variables = master_variables_copy.merge(vulnerability[['timeperiod', 'object_id', 'efficiency', 'vulnerability','landd_score']],
                       on = ['timeperiod', 'object_id'])

master_variables.to_csv(os.getcwd()+'/IDS-DRR-Assam/RiskScoreModel/data/factor_scores_l1_vulnerability_weighted.csv', index=False)

