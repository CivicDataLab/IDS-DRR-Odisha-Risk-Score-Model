#!/usr/bin/env python3
# vulnerability-landd-weight.py

import os
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import jenkspy
import DEA

# ----------------------------------------------------------------------
# 1. CONFIG
# ----------------------------------------------------------------------
DATA_DIR = Path("data/bihar")
IN_FILE  = '/home/prajna/civicdatalab/ids-drr/bihar/flood-data-ecosystem-Bihar/MASTER_VARIABLES.csv'
OUT_FILE = DATA_DIR / "factor_scores_l1_vulnerability_weighted.csv"

VULNERABILITY_VARS = [
    # "mean_sex_ratio", "schools_count", "HealthCenters", "rail_length",
     "avg_electricity",
    "sd_piped_hhds_pct", "sd_nosanitation_hhds_pct",
    # "sum_aged_population",
]

DAMAGE_VARS = [
    # "Total_No_Of_Death_of_Humans_In_Flood_and_Cyclone",
    # "Population_Affected", "Cultivated_Area_affected_in_Hectare",
    # "road_length",
]

NEG_INPUTS   = ["sd_nosanitation_hhds_pct"]
GOOD_INPUTS  = [
    # "schools_count", "HealthCenters", "rail_length",
    "avg_electricity", "sd_piped_hhds_pct",
]

# ----------------------------------------------------------------------
# 2. LOAD & NORMALISE
# ----------------------------------------------------------------------
mv = pd.read_csv(IN_FILE).loc[:, ~pd.read_csv(IN_FILE).columns.duplicated()].copy()

# mv["Total_Livestock_Lost"]                        /= mv["sum_population"]
# mv["Total_No_Of_Death_of_Humans_In_Flood_and_Cyclone"] /= mv["sum_population"]
# mv["House_Damage_Total"]                          /= mv["sum_population"]

# mv["Cultivated_Area_affected_in_Hectare"]         /= mv["net_sown_area_in_hac"]
# mv["road_length"]                                 /= mv["block_area"]
# mv["sum_aged_population"]                         /= mv["block_area"]
# mv["rail_length"]                                 /= mv["block_area"]
# mv["schools_count"]                               /= mv["block_area"]
# mv["HealthCenters"]                               /= mv["block_area"]
# mv["net_sown_area_in_hac"]                        /= mv["block_area"]

# ----------------------------------------------------------------------
# 3. WORKING DF + LANDD SCORE
# ----------------------------------------------------------------------
vul = mv[VULNERABILITY_VARS + DAMAGE_VARS + ["timeperiod", "object_id"]].copy()
vul[DAMAGE_VARS + NEG_INPUTS] = vul[NEG_INPUTS].astype("float64")

scaler = MinMaxScaler()
vul["landd_score"] = scaler.fit_transform(vul[NEG_INPUTS]).sum(axis=1) + 1

# ---- helper to keep weighted damages ≤ 1 --------------------------------
def weight_and_clip(df, cols, w):
    df.loc[:, cols] = (df.loc[:, cols].mul(w, axis=0)).clip(upper=1)
    return df

# ----------------------------------------------------------------------
# 4. APPLY CUSTOM WEIGHTS (vectorised, clipped)
# ----------------------------------------------------------------------
mask = vul[DAMAGE_VARS].gt(0.001).any(axis=1)
weights = vul["landd_score"].pow(2)
vul = weight_and_clip(vul, DAMAGE_VARS + NEG_INPUTS, weights.where(mask, 1.0))

# ----------------------------------------------------------------------
# 5. DEA + CLASSIFICATION PER MONTH
# ----------------------------------------------------------------------
out_frames = []
for month in tqdm(vul["timeperiod"].unique(), desc="DEA per month"):
    df_m = vul[vul["timeperiod"] == month].copy().set_index("object_id")

    # 5-a. 0–1 scale
    df_m[VULNERABILITY_VARS + DAMAGE_VARS] = scaler.fit_transform(
        df_m[VULNERABILITY_VARS + DAMAGE_VARS]
    )

    # 5-b. reverse “good” inputs
    df_m[GOOD_INPUTS] = 1 - df_m[GOOD_INPUTS]

    # 5-c. reverse outputs and clip to [0,1]
    df_m[DAMAGE_VARS] = (1 - df_m[DAMAGE_VARS]).clip(lower=0)

    # 5-d. DEA
    X, y = df_m[VULNERABILITY_VARS].T.to_dict("list"), df_m[DAMAGE_VARS].T.to_dict("list")
    eff  = DEA.CRS(df_m.index.tolist(), X, y, orientation="input", dual=False)
    df_m = df_m.reset_index().merge(eff, left_on="object_id", right_on="DMU")

    # 5-e. clean efficiency column
    df_m["efficiency"] = (
        pd.to_numeric(df_m["efficiency"], errors="coerce")
          .replace([np.inf, -np.inf], np.nan)
    )
    df_m.dropna(subset=["efficiency"], inplace=True)

    # 5-f. Jenks (or fallback to quantiles)
    eff_vals = df_m["efficiency"].values.astype("float64")
    if len(np.unique(eff_vals)) >= 5:
        breaks = jenkspy.jenks_breaks(eff_vals, n_classes=5)
        df_m["vulnerability"] = pd.cut(eff_vals, breaks,
                                       labels=[5, 4, 3, 2, 1], include_lowest=True, duplicates='drop')
    else:                                              # too few distinct values
        df_m["vulnerability"] = pd.qcut(eff_vals, q=5, labels=[5, 4, 3, 2, 1], duplicates='drop')

    out_frames.append(df_m)

vulnerability = pd.concat(out_frames, ignore_index=True)

# ----------------------------------------------------------------------
# 6. MERGE BACK & SAVE
# ----------------------------------------------------------------------
final = mv.merge(
    vulnerability[["timeperiod", "object_id", "efficiency", "vulnerability", "landd_score"]],
    on=["timeperiod", "object_id"],
    how="left"
)
final.to_csv(OUT_FILE, index=False)
print(f"✓  Saved {OUT_FILE.relative_to(Path.cwd())}")
